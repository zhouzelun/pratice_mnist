{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import minist_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "MODEL_SAVE_PATH=\"/Users/zhouzelun/Documents/python/mnist_data\"\n",
    "MODEL_NAME = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32,[None,minist_inference.INPUT_NODE],name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32,[None,minist_inference.OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    #正则项\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    #计算未使用滑动平均一次前向传播结果\n",
    "    y = minist_inference.inference(x,regularizer)\n",
    "    \n",
    "    #定义当前步数，移动平均时会用到，自动更新+1\n",
    "    global_step = tf.Variable(0,trainable = False)\n",
    "    #计算使用滑动平均的前向传播结果\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    #在前向传播过后计算交叉熵\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    #交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    #损失等于交叉商加上正则项\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    #定义学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,#基础学习速率\n",
    "                                              global_step,       #当前迭代轮数\n",
    "                                              500,  #总共需要的迭代次数\n",
    "                                              LEARNING_RATE_DECAY)      #学习率衰减速率\n",
    "    #训练过程\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "                    .minimize(loss,global_step = global_step)\n",
    "    \n",
    "    train_op = tf.group(train_step,variables_averages_op) \n",
    "    #保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "            if i%1000==0:\n",
    "                print(\"After %d training step(s),loss on training batch is %g.\" %(step,loss_value))\n",
    "                \n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/zhouzelun/Documents/python/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/zhouzelun/Documents/python/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/zhouzelun/Documents/python/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zhouzelun/Documents/python/mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s),loss on training batch is 2.80674.\n",
      "After 1001 training step(s),loss on training batch is 0.21578.\n",
      "After 2001 training step(s),loss on training batch is 0.146552.\n",
      "After 3001 training step(s),loss on training batch is 0.169459.\n",
      "After 4001 training step(s),loss on training batch is 0.132025.\n",
      "After 5001 training step(s),loss on training batch is 0.108022.\n",
      "After 6001 training step(s),loss on training batch is 0.103201.\n",
      "After 7001 training step(s),loss on training batch is 0.0890887.\n",
      "After 8001 training step(s),loss on training batch is 0.0872735.\n",
      "After 9001 training step(s),loss on training batch is 0.0813391.\n",
      "After 10001 training step(s),loss on training batch is 0.0765231.\n",
      "After 11001 training step(s),loss on training batch is 0.0642342.\n",
      "After 12001 training step(s),loss on training batch is 0.0614186.\n",
      "After 13001 training step(s),loss on training batch is 0.0597294.\n",
      "After 14001 training step(s),loss on training batch is 0.0539472.\n",
      "After 15001 training step(s),loss on training batch is 0.0473602.\n",
      "After 16001 training step(s),loss on training batch is 0.0455008.\n",
      "After 17001 training step(s),loss on training batch is 0.0460288.\n",
      "After 18001 training step(s),loss on training batch is 0.0443229.\n",
      "After 19001 training step(s),loss on training batch is 0.0429991.\n",
      "After 20001 training step(s),loss on training batch is 0.0441905.\n",
      "After 21001 training step(s),loss on training batch is 0.0398438.\n",
      "After 22001 training step(s),loss on training batch is 0.0413847.\n",
      "After 23001 training step(s),loss on training batch is 0.038488.\n",
      "After 24001 training step(s),loss on training batch is 0.0375577.\n",
      "After 25001 training step(s),loss on training batch is 0.0350412.\n",
      "After 26001 training step(s),loss on training batch is 0.0389722.\n",
      "After 27001 training step(s),loss on training batch is 0.0339461.\n",
      "After 28001 training step(s),loss on training batch is 0.0342555.\n",
      "After 29001 training step(s),loss on training batch is 0.0367499.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv = None):\n",
    "    mnist = input_data.read_data_sets(MODEL_SAVE_PATH,one_hot = True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
